r"""Generate absolute evaluation judgements for responses generated by a candidate model.
"""

import ast
import re
import sys
from typing import Any, Dict, List, Optional, Tuple

import models
from absl import app, flags

sys.path.append("contexteval/common")
import jsonl_utils
import tqdm
import tsv_utils

_CONTEXT_NAME = flags.DEFINE_string("context_name", "", "Name of contextual attribute.")
_EVAL_MODEL = flags.DEFINE_string("eval_model", "gemini-1.5-pro", "Model to use for evaluation.")
_DEFAULT = flags.DEFINE_bool(
    "default",
    True,
    "Generate default response.",
)


def extract_dictionary(input_string: str) -> Optional[Dict[str, Any]]:
    try:
        input_string = input_string.replace("*", "").strip()
        search_str = "output: "
        end_str = "}"
        dict_start = input_string.find(search_str) + len(search_str)
        dict_end = input_string.find(end_str) + 1
        dict_str = input_string[dict_start:dict_end]
        dictionary = ast.literal_eval(dict_str)
        return dictionary
    except Exception:
        print(input_string)
        print("Error occurred while extracting dictionary")
        return None


def get_qa(input_str: str) -> Tuple[str, List[str]]:
    """Extract question and answer list from input string.

    Args:
        input_str: String containing Q: and A: sections

    Returns:
        Tuple of (question, answer_list)

    Raises:
        ValueError: If question or answer list cannot be found
    """
    question_match = re.search(r"Q: (.*?) A:", input_str)
    if not question_match:
        raise ValueError("Question not found in input string")
    question = question_match.group(1)

    list_match = re.search(r"A: (\[.*\])", input_str)
    if not list_match:
        raise ValueError("Answer list not found in input string")
    answer_list_str = list_match.group(1)
    try:
        answer_list = ast.literal_eval(answer_list_str)
        if not isinstance(answer_list, list):
            raise ValueError("Answer list is not a valid list")
        return question, answer_list
    except (SyntaxError, ValueError) as e:
        raise ValueError(f"Failed to parse answer list: {e}")


def get_eval_judgement(cur_prompt: str, model: Any) -> str:
    """Get evaluation judgement from model with retries.

    Args:
        cur_prompt: Prompt to send to model
        model: Model instance to use for generation

    Returns:
        Model's evaluation judgement as string
    """
    max_retries = 3
    for attempt in range(max_retries):
        try:
            eval_judgement = model.generate(cur_prompt)
            if eval_judgement:
                return eval_judgement
        except Exception as e:
            if attempt == max_retries - 1:
                raise RuntimeError(
                    f"Failed to get evaluation judgement after {max_retries} attempts: {e}"
                )
            continue
    raise RuntimeError("Failed to get evaluation judgement")


def main(unused_argv) -> None:
    if _DEFAULT.value:
        eval_prompt = "\n".join(tsv_utils.read_txt("prompts/eval_context_analysis_default.txt"))
    else:
        eval_prompt = "\n".join(tsv_utils.read_txt("prompts/eval_context_analysis_adapted.txt"))

    CANDIDATE_PATH = f"data/context_analysis/{_CONTEXT_NAME.value.replace('/', '_').replace(' ', '_')}_responses_gpt-4.jsonl"
    OUTPUT_PATH = f"data/context_analysis/{_CONTEXT_NAME.value.replace('/', '_').replace(' ', '_')}_evals_gpt-4.jsonl"
    if not _DEFAULT.value:
        CANDIDATE_PATH = CANDIDATE_PATH.replace(".jsonl", "_adapted.jsonl")
        OUTPUT_PATH = OUTPUT_PATH.replace(".jsonl", "_adapted.jsonl")

    candidate_outputs = jsonl_utils.read(CANDIDATE_PATH)
    contextual_attributes = tsv_utils.read_txt("data/contextual_attributes_qas.txt")
    context_to_question = {}
    for line in contextual_attributes:
        if line[0] == "*":
            cur_context = line[1:].strip()
        else:
            context_to_question[cur_context] = line.strip()

    if "gpt" in _EVAL_MODEL.value:
        model = models.GPT4(model_name=_EVAL_MODEL.value)
    elif "gemini" in _EVAL_MODEL.value:
        model = models.Gemini(model_name=_EVAL_MODEL.value)
    elif "claude" in _EVAL_MODEL.value:
        model = models.Claude(model_name=_EVAL_MODEL.value)
    elif _EVAL_MODEL.value == "llama-3.1":
        model = models.TogetherAI()
    elif "jamba" in _EVAL_MODEL.value:
        model = models.Jamba(model_name=_EVAL_MODEL.value)
    else:
        model = models.TogetherAI(model_name=_EVAL_MODEL.value)

    outputs = []
    for ex in tqdm.tqdm(candidate_outputs):
        if "context" in ex:
            # Adapted response
            cur_prompt = (eval_prompt + ".")[:-1]
            cur_prompt = cur_prompt.replace("[QUERY]", ex["query"])
            cur_prompt = cur_prompt.replace("[RESPONSE]", ex["response"])
            cur_prompt = cur_prompt.replace("[CONTEXT]", ex["context"])
            eval_judgement = get_eval_judgement(cur_prompt, model)
            outputs.append(
                {
                    "query": ex["query"],
                    "response": ex["response"],
                    "context": ex["context"],
                    "eval_judgement": eval_judgement,
                    "is_default_response": False,
                }
            )
            jsonl_utils.write(OUTPUT_PATH, [outputs[-1]], append=True, verbose=False)
        else:
            # Default response
            question, answer_list = get_qa(context_to_question[_CONTEXT_NAME.value])
            output_format = (
                "**output: "
                + str({k: {"relevance": "_"} for k in answer_list}).replace("'_'", "_")
                + "**"
            )
            cur_prompt = (eval_prompt + ".")[:-1]
            cur_prompt = cur_prompt.replace("[OUTPUT_FORMAT]", output_format)
            context = context_to_question[_CONTEXT_NAME.value]
            cur_prompt = cur_prompt.replace("[CONTEXT]", context)
            cur_prompt = cur_prompt.replace("[QUERY]", ex["query"])
            cur_prompt = cur_prompt.replace("[RESPONSE]", ex["response"])
            eval_judgement = get_eval_judgement(cur_prompt, model)
            outputs.append(
                {
                    "query": ex["query"],
                    "response": ex["response"],
                    "context": context,
                    "eval_judgement": eval_judgement,
                    "is_default_response": True,
                }
            )
            jsonl_utils.write(OUTPUT_PATH, [outputs[-1]], append=True, verbose=False)


if __name__ == "__main__":
    app.run(main)
