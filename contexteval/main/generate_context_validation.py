r"""Generate context type for a follow-up QA.

Example usage:

INPUT_PATH=data/sampled_qa_contexts_gpt-4.jsonl
OUTPUT_PATH=data/sampled_qa_contexts_with_validation.jsonl
python3.10 main/generate_context_validation.py \
--input_path=${INPUT_PATH} \
--output_path=${OUTPUT_PATH}
"""

from absl import app
from absl import flags

import ast
import collections
import models
import random
import re
import sys
sys.path.append('contexteval/common')
import example_utils
import tsv_utils
import tqdm

random.seed(42)


_INPUT_PATH = flags.DEFINE_string(
    "input_path", "", "Path to the input file."
)
_OUTPUT_PATH = flags.DEFINE_string(
    "output_path", "", "Path to the output file."
)

MODEL_ONE="gpt-4"
MODEL_TWO="gemini-1.5-pro"
MODEL_THREE="claude-3.5-sonnet"


def extract_dictionary(input_string):
  try:
    input_string = input_string.replace("*", "").strip()
    search_str = "output: "
    end_str = "}"
    dict_start = input_string.find(search_str) + len(search_str)
    dict_end = input_string.find(end_str) + 1
    dict_str = input_string[dict_start:dict_end]
    dictionary = ast.literal_eval(dict_str)
    return dictionary
  except Exception as e:
    return None

  
def check_output_format(input_string):
  try:
    input_string = input_string.replace("*", "").strip()
    search_str = "output: "
    end_str = "}}"
    dict_start = input_string.find(search_str) + len(search_str)
    dict_end = input_string.find(end_str) + 2
    dict_str = input_string[dict_start:dict_end]
    input_string = ast.literal_eval(dict_str)
    if not isinstance(input_string, dict):
      return False
    
    # Define the required keys for each question
    required_keys = {"important", "realistic", "complete", "diverse"}
    
    for key, value in input_string.items():
      # Check if each key follows the "Qx" format
      if not key.startswith("Q") or not key[1:].isdigit():
        return False
        
      # Check if the value is a dictionary
      if not isinstance(value, dict):
        return False
        
      # Check if the dictionary contains the required keys
      if set(value.keys()) != required_keys:
        return False
        
      # Check if the values for each required key are integers and equal to 1
      for k, v in value.items():
        if not isinstance(v, int) or (v != 0 and v != 1):
          return False
    return True

  except Exception as e:
    print(input_string)
    print(f"Error occurred: {e}")
    return False
  
def get_validation_labels(prompt, model):
  trials = 0
  while True:
    out_labels = model.generate(prompt)
    is_valid_output = check_output_format(out_labels)
    trials += 1
    if is_valid_output or trials == 3:
      break
  return extract_dictionary(out_labels)


def main(unused_argv) -> None:
  context_prompt = "\n".join(tsv_utils.read_txt("prompts/context_validation_all_prompt.txt"))
  examples = {}
  examples[MODEL_ONE] = example_utils.read_examples(_INPUT_PATH.value)
  examples[MODEL_TWO] = example_utils.read_examples(_INPUT_PATH.value.replace(MODEL_ONE, MODEL_TWO))
  examples[MODEL_THREE] = example_utils.read_examples(_INPUT_PATH.value.replace(MODEL_ONE, MODEL_THREE))
  underspec_queries = {}
  underspec_queries[MODEL_ONE] = [ex.query for ex in examples[MODEL_ONE] if ex.contexts != "No" and "Need for Context: No" not in ex.contexts]
  underspec_queries[MODEL_TWO] = [ex.query for ex in examples[MODEL_TWO] if ex.contexts != "No" and "Need for Context: No" not in ex.contexts]
  underspec_queries[MODEL_THREE] = [ex.query for ex in examples[MODEL_THREE] if ex.contexts != "No" and "Need for Context: No" not in ex.contexts]
  all_underspec_queries = set(underspec_queries[MODEL_ONE] + underspec_queries[MODEL_TWO] + underspec_queries[MODEL_THREE])

  indices = list(range(0, len(examples[MODEL_ONE])))
  eval_models = {}
  eval_models[MODEL_ONE] = models.GPT4()
  eval_models[MODEL_TWO] = models.Gemini()
  eval_models[MODEL_THREE] = models.Claude(model_name=MODEL_THREE)
  model_idxs = random.choices([1, 2, 3], k=len(indices))

  for idx in tqdm.tqdm(indices):
    # Skip examples that are not underspecified
    if examples[MODEL_ONE][idx].query not in all_underspec_queries:
      continue
    model_idx = model_idxs[indices.index(idx)]
    skip = True
    tries = 0
    ex = None
    # Pick context generated by one of the three models
    while skip==True:
      if model_idx == 1:
        ex = examples[MODEL_ONE][idx]
      elif model_idx == 2:
        ex = examples[MODEL_TWO][idx]
      elif model_idx == 3:
        ex = examples[MODEL_THREE][idx]

      questions = [q.strip() for q in re.findall(r'Q:.*?\?', ex.contexts)]
      answers = [a.strip() for a in re.findall(r'A:.*?\]', ex.contexts)]

      if len(questions) != len(answers):
        model_idx = random.choice([1, 2, 3])
      else:
        skip = False
        break
      if tries == 10:
        break
      tries += 1
    
    if skip:
      continue

    cur_prompt = (context_prompt + ".")[:-1]
    cur_prompt = cur_prompt.replace("[QUERY]", ex.query)
    cur_prompt = cur_prompt.replace("[QUESTIONS]", "\n".join([f"Q{i+1}: {q}\nA{i+1}: {a}" for i, (q, a) in enumerate(zip(questions, answers))]))
    
    val_labels = {}
    val_labels[MODEL_ONE] = get_validation_labels(cur_prompt, eval_models[MODEL_ONE])
    val_labels[MODEL_TWO] = get_validation_labels(cur_prompt, eval_models[MODEL_TWO])
    val_labels[MODEL_THREE] = get_validation_labels(cur_prompt, eval_models[MODEL_THREE])
    selected_qas = []
    for q_idx in range(1, len(questions)+1):
      validation_labels = [val_labels[MODEL_ONE][f"Q{q_idx}"], val_labels[MODEL_TWO][f"Q{q_idx}"], val_labels[MODEL_THREE][f"Q{q_idx}"]]
      counter = collections.Counter([labels["important"] for labels in validation_labels if labels is not None and "important" in labels])
      majority_label, _ = counter.most_common(1)[0]
      if majority_label == 0:
        continue
      selected_qas.append((questions[q_idx], answers[q_idx]))
  
    ex.contexts = "\n".join([f"Q{i+1}: {q}\nA{i+1}: {a}" for i, (q, a) in enumerate(selected_qas)])
    ex.context_model_source = model_idx # model source (1: GPT-4, 2: Gemini, 3: Claude)

    example_utils.write_examples(_OUTPUT_PATH.value, [ex], append=True)


if __name__ == "__main__":
  app.run(main)
